{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CBA 2020\n",
    "## Detecção de anomalias em poços produtores de petróleo usando aprendizado de máquina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**_Principais Referências_**\n",
    "  \n",
    "- **A Realistic and Public Dataset with Rare Undesirable Real Events in Oil Wells** ([link](https://doi.org/10.1016/j.petrol.2019.106223)).\n",
    "- **Github de referência do _benchmark_ proposto por Vargas (2019)** ([link](https://github.com/ricardovvargas/3w_dataset))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Regras do benchmark proposto por Vargas (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Apenas instâncias reais com anomalias de tipos que têm períodos normais maiores ou iguais a vinte minutos foram utilizadas;\n",
    "\n",
    "- Múltiplas rodadas de treinamento e validação realizadas, sendo o número de rodadas igual ao número de instâncias. Em cada rodada, amostras utilizadas para treinamento ou validação extraídas de apenas uma instância. Parte das amostras de normalidade utilizadas no treinamento (60%) e a outra parte para teste (40%). As amostras de anomalias somente devem ser utilizadas apenas teste, sendo, portanto, uma técnica de treinamento de classe única. O conjunto de teste deve ser composto pelo mesmo número de amostras de cada classe (normalidade e anormalidade);\n",
    "\n",
    "- Em cada rodada, precisão, revogação e medida F1 devem ser computadas (valor médio e desvio padrão de cada métrica), sendo o valor médio da medida F1 considerado a principal métrica de desempenho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1 Outras definições adotadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Uma estratégia de amostragem específica com janela deslizante foi usada para cada tipo de período. Em períodos normais, as primeiras observações são usadas para treinamento e as últimas são usadas para testes. Em períodos transientes, procura-se usar observações como um todo (apenas para teste). Em períodos de regime, as primeiras observações são privilegiadas (somente para teste);\n",
    "\n",
    "- Antes de cada rodada de treinamento e teste:\n",
    "    - As amostras utilizadas (não as instâncias) são adequadamente normalizadas com o z-score;\n",
    "    - As variáveis de amostras (não as instâncias) usadas para treinamento que possuem um número de NaNs acima de um limite ou que têm um desvio padrão abaixo de um outro limite são descartadas.\n",
    "\n",
    "- Todos os random_state necessários são atribuídos a uma constante para que os resultados sejam reproduzíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Bibliotecas e configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Artifício para alcular tempo total do notebook Jupyter\n",
    "from datetime import datetime \n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('stac')\n",
    "import nonparametric_tests as stac\n",
    "from math import ceil\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger('tsfresh').setLevel(logging.ERROR)\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = Path('./', 'data')\n",
    "random_state = 1\n",
    "events_names = {0: 'Normal',\n",
    "                1: 'Aumento Abrupto de BSW',\n",
    "                2: 'Fechamento Espúrio de DHSV',\n",
    "                3: 'Intermitência Severa',\n",
    "                4: 'Instabilidade de Fluxo',\n",
    "                5: 'Perda Rápida de Produtividade',\n",
    "                6: 'Restrição Rápida em CKP',\n",
    "                7: 'Incrustação em CKP',\n",
    "                8: 'Hidrato em Linha de Produção'\n",
    "               }\n",
    "vars = ['P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        'P-JUS-CKGL',\n",
    "        'T-JUS-CKGL',\n",
    "        'QGL']\n",
    "columns = ['timestamp'] + vars + ['class'] \n",
    "normal_class_code = 0\n",
    "abnormal_classes_codes = [1, 2, 5, 6, 7, 8]\n",
    "sample_size = 3*60              # Nas observações = segundos\n",
    "min_normal_period_size = 20*60  # Nas observações = segundos\n",
    "split_range = 0.6               # Porcentagem de separação entre treino/teste\n",
    "max_samples_per_period = 15     # limitação por 'segurança'\n",
    "df_fc_p = MinimalFCParameters() # Ver documentação da biblioteca tsfresh - opção: EfficientFCParameters()\n",
    "df_fc_p.pop('sum_values')       # Remove feature inapropriada\n",
    "df_fc_p.pop('length')           # Remove feature inapropriada\n",
    "max_nan_percent = 0.1           # Para seleção de variáveis\n",
    "std_vars_min = 0.01             # Para seleção de variáveis\n",
    "clfs = {}                       # Dicionário para lista de classificadores a serem experimentados\n",
    "disable_progressbar = True      # Para menos saídas no notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def class_and_file_generator(data_path, real=False, simulated=False, drawn=False):\n",
    "    \"\"\"Gerador de lista contendo número da classe e caminho do arquivo de acordo com a fonte da instância.\"\"\"    \n",
    "    for class_path in data_path.iterdir():\n",
    "        if class_path.is_dir():\n",
    "            class_code = int(class_path.stem)\n",
    "            for instance_path in class_path.iterdir():\n",
    "                if (instance_path.suffix == '.csv'):\n",
    "                    if (simulated and instance_path.stem.startswith('SIMULATED')) or \\\n",
    "                       (drawn and instance_path.stem.startswith('DRAWN')) or \\\n",
    "                       (real and (not instance_path.stem.startswith('SIMULATED')) and \\\n",
    "                       (not instance_path.stem.startswith('DRAWN'))):\n",
    "                        yield class_code, instance_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_instance(instance_path):\n",
    "    \"\"\"Função que carrega cada instância individualmente\"\"\"\n",
    "    try:\n",
    "        well, instance_id = instance_path.stem.split('_')\n",
    "        df = pd.read_csv(instance_path, sep=',', header=0)\n",
    "        assert (df.columns == columns).all(), \\\n",
    "            f'Colunas inválidas no arquivo {str(instance_path)}: {str(df.columns.tolist())}'\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Exception(f'Erro ao ler arquivo {instance_path}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_samples(df, class_code):\n",
    "    # Obtém os rótulos das observações e seu conjunto inequívoco\n",
    "    ols = list(df['class'])\n",
    "    set_ols = set()\n",
    "    for ol in ols:\n",
    "        if ol in set_ols or np.isnan(ol):\n",
    "            continue\n",
    "        set_ols.add(int(ol))       \n",
    "    \n",
    "    # Descarta os rótulos das observações e substitui todos os nan por 0\n",
    "    # (requisito da biblioteca tsfresh)\n",
    "    df_vars = df.drop('class', axis=1).fillna(0)  \n",
    "    \n",
    "    # Inicializa objetos que serão retornados\n",
    "    df_samples_train = pd.DataFrame()\n",
    "    df_samples_test = pd.DataFrame()\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "            \n",
    "    # Descubre o número máximo de amostras em períodos normais, transitórios e em regime\n",
    "    # Obtém índices (primeiro e último) sem sobreposição com outros períodos\n",
    "    f_idx = ols.index(normal_class_code)\n",
    "    l_idx = len(ols)-1-ols[::-1].index(normal_class_code)\n",
    "\n",
    "    # Define o número inicial de amostras para o período normal\n",
    "    max_samples_normal = l_idx-f_idx+1-sample_size\n",
    "    if (max_samples_normal) > 0:      \n",
    "        num_normal_samples = min(max_samples_per_period, max_samples_normal)\n",
    "        num_train_samples = int(split_range*num_normal_samples)\n",
    "        num_test_samples = num_normal_samples - num_train_samples    \n",
    "    else:\n",
    "        num_train_samples = 0\n",
    "        num_test_samples = 0\n",
    "    \n",
    "    # Define o número máximo de amostras por período transitório\n",
    "    transient_code = class_code + 100    \n",
    "    if transient_code in set_ols:\n",
    "        # Obtém índices (primeiro e último) com possível sobreposição\n",
    "        # no início do período\n",
    "        f_idx = ols.index(transient_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(transient_code)        \n",
    "        max_transient_samples = l_idx-f_idx+1-sample_size\n",
    "    else:\n",
    "        max_transient_samples = 0            \n",
    "\n",
    "    # Define o número máximo de amostras no período de regime\n",
    "    if class_code in set_ols:\n",
    "        # Obtém índices (primeiro e último) com possível sobreposição \n",
    "        # no início ou fim do período\n",
    "        f_idx = ols.index(class_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(class_code)\n",
    "        if l_idx+(sample_size-1) < len(ols)-1:\n",
    "            l_idx = l_idx+(sample_size-1) \n",
    "        else:\n",
    "            l_idx = len(ols)-1\n",
    "        max_in_regime_samples = l_idx-f_idx+1-sample_size\n",
    "    else:\n",
    "        max_in_regime_samples = 0   \n",
    "        \n",
    "    # Descubre o número adequado de amostras em períodos normais, transitórios e em regime\n",
    "    num_transient_samples = ceil(num_test_samples/2)\n",
    "    num_in_regime_samples = num_test_samples - num_transient_samples\n",
    "    if (max_transient_samples >= num_transient_samples) and \\\n",
    "       (max_in_regime_samples < num_in_regime_samples):\n",
    "        num_in_regime_samples = max_in_regime_samples        \n",
    "        num_transient_samples = min(num_test_samples-num_in_regime_samples, max_transient_samples)\n",
    "    elif (max_transient_samples < num_transient_samples) and \\\n",
    "         (max_in_regime_samples >= num_in_regime_samples):\n",
    "        num_transient_samples = max_transient_samples        \n",
    "        num_in_regime_samples = min(num_test_samples-num_transient_samples, max_in_regime_samples)\n",
    "    elif (max_transient_samples < num_transient_samples) and \\\n",
    "         (max_in_regime_samples < num_in_regime_samples):\n",
    "        num_transient_samples = max_transient_samples\n",
    "        num_in_regime_samples = max_in_regime_samples\n",
    "        num_test_samples = num_transient_samples+num_in_regime_samples\n",
    "    \n",
    "    # Extrai amostras do período normal para treinamento e teste\n",
    "    # Obtém índices (primeiro e último) sem sobreposição com outros períodos\n",
    "    f_idx = ols.index(normal_class_code)\n",
    "    l_idx = len(ols)-1-ols[::-1].index(normal_class_code)\n",
    "    \n",
    "    # Define a etapa correta e extrai amostras\n",
    "    if (num_normal_samples) > 0:  \n",
    "        if num_normal_samples == max_samples_normal:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_samples_normal-1) // (max_samples_per_period-1)\n",
    "        step_wanted = sample_size\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Extrai amostras para treinamento\n",
    "        sample_id = 0\n",
    "        for idx in range(num_train_samples):\n",
    "            f_idx_c = l_idx-sample_size+1-(num_normal_samples-1-idx)*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_train = df_samples_train.append(df_sample)\n",
    "            y_train.append(normal_class_code)\n",
    "            sample_id += 1\n",
    "    \n",
    "        # Extrai amostras para teste\n",
    "        sample_id = 0\n",
    "        for idx in range(num_train_samples, num_train_samples+num_test_samples):\n",
    "            f_idx_c = l_idx-sample_size+1-(num_normal_samples-1-idx)*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(normal_class_code)\n",
    "            sample_id += 1\n",
    "\n",
    "    # Extrai amostras do período transitório (se existir) para teste\n",
    "    if (num_transient_samples) > 0:    \n",
    "        # Define a etapa correta e extrai amostras\n",
    "        if num_transient_samples == max_transient_samples:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_transient_samples-1) // (max_samples_per_period-1)\n",
    "        step_wanted = np.inf\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Obtém índices (primeiro e último) com possível sobreposição no início deste período\n",
    "        f_idx = ols.index(transient_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(transient_code) \n",
    "\n",
    "        # Extrai amostras\n",
    "        for idx in range(num_transient_samples):\n",
    "            f_idx_c = f_idx+idx*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(transient_code)\n",
    "            sample_id += 1\n",
    "            \n",
    "    # Extrai amostras do período em regime (se existir) para teste\n",
    "    if (num_in_regime_samples) > 0:     \n",
    "        # Define a etapa correta e extrai amostras\n",
    "        if num_in_regime_samples == max_in_regime_samples:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_in_regime_samples-1) // (max_samples_per_period-1)\n",
    "        step_wanted = sample_size\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Obtém índices (primeiro e último) com possível sobreposição \n",
    "        # no início ou no final deste período\n",
    "        f_idx = ols.index(class_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(class_code)\n",
    "        if l_idx+(sample_size-1) < len(ols)-1:\n",
    "            l_idx = l_idx+(sample_size-1) \n",
    "        else:\n",
    "            l_idx = len(ols)-1\n",
    "\n",
    "        # Extrai amostras\n",
    "        for idx in range(num_in_regime_samples):\n",
    "            f_idx_c = f_idx+idx*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(class_code)\n",
    "            sample_id += 1\n",
    "            \n",
    "    #print(f'df_samples_train (antes de normalizar):')\n",
    "    #display(df_samples_train)\n",
    "    #print(f'y_train (antes de ajustar para +1 e -1): {y_train} \\n')\n",
    "    #print(f'df_samples_test (antes de normalizar):')\n",
    "    #display(df_samples_test)\n",
    "    #print(f'y_test (antes de ajustar para +1 e -1): {y_test} \\n')\n",
    "    \n",
    "    return df_samples_train, y_train, df_samples_test, y_test              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test_calc_scores(X_train, y_train, X_test, y_test, scores, clfs):\n",
    "    X_train.reset_index(inplace=True, drop=True)\n",
    "    X_test.reset_index(inplace=True, drop=True)    \n",
    "    for clf_name, clf in clfs.items():\n",
    "        #print(f'CLASSIFICADOR: {clf_name}')\n",
    "        #print(f'y_train: {y_train}')\n",
    "        #print(f'y_test: {y_test}')\n",
    "        \n",
    "        # Treino\n",
    "        t0 = time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        t_train = time() - t0\n",
    "\n",
    "        # Teste\n",
    "        t0 = time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        #print(f'y_pred: {y_pred}')\n",
    "        t_test = time() - t0\n",
    "\n",
    "        # Plota os labels reais e preditos pelo classificador\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(12,1))\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(-(y_pred), marker=11, color='orange', linestyle='') # Ordem invertida (mais natural)\n",
    "        plt.plot(-(y_test), marker=10, color='green', linestyle='')  # Ordem invertida (mais natural)\n",
    "        ax.grid(False)\n",
    "        ax.set_yticks([-1, 1])\n",
    "        ax.set_yticklabels(['Normal', 'Anormal'])\n",
    "        ax.set_title(clf_name)            \n",
    "        ax.set_xlabel('Amostra')\n",
    "        ax.legend(['Classe Prevista', 'Classe Real'])\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        \n",
    "        # Calcula as metricas de desempenho\n",
    "        ret = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "        p, r, f1, _ = ret\n",
    "        scores = scores.append({'CLASSIFICADOR': clf_name, \n",
    "                                'PRECISAO': p,\n",
    "                                'REVOGACAO': r,\n",
    "                                'F1': f1,\n",
    "                                'TREINAMENTO [s]': t_train, \n",
    "                                'TESTE [s] ': t_test}, ignore_index=True)  \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gets all real instances but maintains only those with any type of undesirable event\n",
    "real_instances = pd.DataFrame(class_and_file_generator(data_path, \n",
    "                                                       real=True,\n",
    "                                                       simulated=False, \n",
    "                                                       drawn=False),\n",
    "                              columns=['class_code', 'instance_path'])\n",
    "real_instances = real_instances.loc[real_instances.iloc[:,0].isin(abnormal_classes_codes)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________\n",
      "Instância 1: data\\1\\WELL-00001_20140124213136.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (959)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 2: data\\1\\WELL-00002_20140126200050.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (1138)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 3: data\\1\\WELL-00006_20170801063614.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 4: data\\1\\WELL-00006_20170802123000.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 5: data\\1\\WELL-00006_20180618060245.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 6: data\\2\\WELL-00002_20131104014101.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 7: data\\2\\WELL-00003_20141122214325.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 8: data\\2\\WELL-00003_20170728150240.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 9: data\\2\\WELL-00003_20180206182917.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (586)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 10: data\\2\\WELL-00009_20170313160804.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 11: data\\2\\WELL-00010_20171218200131.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 12: data\\2\\WELL-00011_20140515110134.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 13: data\\2\\WELL-00011_20140530100015.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (482)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 14: data\\2\\WELL-00011_20140606230115.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 15: data\\2\\WELL-00011_20140720120102.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 16: data\\2\\WELL-00011_20140726180015.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (900)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 17: data\\2\\WELL-00011_20140824000118.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 18: data\\2\\WELL-00011_20140916060300.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 19: data\\2\\WELL-00011_20140921200031.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (695)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 20: data\\2\\WELL-00011_20140928100056.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 21: data\\2\\WELL-00011_20140929170028.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (975)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 22: data\\2\\WELL-00011_20140929220121.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 23: data\\2\\WELL-00011_20141005170056.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 24: data\\2\\WELL-00011_20141006160121.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 25: data\\2\\WELL-00012_20170320033022.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (773)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 26: data\\2\\WELL-00012_20170320143144.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 27: data\\2\\WELL-00013_20170329020229.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 28: data\\5\\WELL-00015_20170620160349.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 29: data\\5\\WELL-00015_20171013140047.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 30: data\\5\\WELL-00016_20180405020345.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (1145)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 31: data\\5\\WELL-00016_20180426142005.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (321)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 32: data\\5\\WELL-00016_20180426145108.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (376)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 33: data\\5\\WELL-00016_20180517222322.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 34: data\\5\\WELL-00017_20140314180000.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (0)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 35: data\\5\\WELL-00017_20140317151743.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 36: data\\5\\WELL-00017_20140318023141.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 37: data\\5\\WELL-00017_20140318160220.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 38: data\\5\\WELL-00017_20140319040453.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 39: data\\5\\WELL-00017_20140319141450.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 40: data\\6\\WELL-00002_20140212170333.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 41: data\\6\\WELL-00002_20140301151700.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 42: data\\6\\WELL-00002_20140325170304.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 43: data\\6\\WELL-00004_20171031181509.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 44: data\\6\\WELL-00004_20171031193025.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (414)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 45: data\\6\\WELL-00004_20171031200059.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (845)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 46: data\\7\\WELL-00001_20170226220309.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 47: data\\7\\WELL-00006_20180618110721.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 48: data\\7\\WELL-00006_20180620181348.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 49: data\\7\\WELL-00018_20180611040207.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (0)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 50: data\\8\\WELL-00019_20170301182317.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n",
      "____________________________________________________________________________________\n",
      "Instância 51: data\\8\\WELL-00020_20120410192326.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (173)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 52: data\\8\\WELL-00021_20170509013517.csv\n",
      "\tDummyClassifier {'assume_centered': False, 'contamination': 0.5, 'random_state': 1, 'support_fraction': 0.99}\n"
     ]
    }
   ],
   "source": [
    "# For each real instance with any type of undesirable event\n",
    "scores = pd.DataFrame()\n",
    "ignored_instances = 0\n",
    "used_instances = 0\n",
    "for i, row in real_instances.iterrows():\n",
    "    # Loads the current instance\n",
    "    class_code, instance_path = row\n",
    "    print(f'____________________________________________________________________________________')        \n",
    "    print(f'Instância {i+1}: {instance_path}')\n",
    "    df = load_instance(instance_path)\n",
    "    \n",
    "    # Ignores instances without sufficient normal periods\n",
    "    normal_period_size = (df['class']==float(normal_class_code)).sum()\n",
    "   \n",
    "    if normal_period_size < min_normal_period_size:\n",
    "        ignored_instances += 1\n",
    "        print(f'\\tignorado porque normal_period_size é insuficiente para treinamento ({normal_period_size})\\n')\n",
    "        continue\n",
    "    used_instances += 1\n",
    "        \n",
    "    # Extracts samples from the current real instance\n",
    "    df_samples_train, y_train, df_samples_test, y_test = extract_samples(df, class_code)\n",
    "    \n",
    "\n",
    "    # Changes types of the labels (tsfresh's requirement)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "   \n",
    "    \n",
    "    # (Wander) incluidas as duas linhas abaixo para ajustar y_train também (?)\n",
    "    y_train[y_train!=normal_class_code] = -1\n",
    "    y_train[y_train==normal_class_code] = 1\n",
    "\n",
    "    # We want binary classification: 1 for inliers (negative class = normal instance) and\n",
    "    # -1 for outliers (positive class = instance with anomaly) (sklearn's requirement)\n",
    "    y_test[y_test!=normal_class_code] = -1\n",
    "    y_test[y_test==normal_class_code] = 1\n",
    "    \n",
    "    # Drops the bad vars\n",
    "    good_vars = np.isnan(df_samples_train[vars]).mean(0) <= max_nan_percent\n",
    "    std_vars = np.nanstd(df_samples_train[vars], 0)\n",
    "    good_vars &= (std_vars > std_vars_min)    \n",
    "    good_vars = list(good_vars.index[good_vars])\n",
    "    bad_vars = list(set(vars)-set(good_vars))\n",
    "    df_samples_train.drop(columns=bad_vars, inplace=True, errors='ignore')\n",
    "    df_samples_test.drop(columns=bad_vars, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Normalizes the samples (zero mean and unit variance)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    df_samples_train[good_vars] = scaler.fit_transform(df_samples_train[good_vars]).astype('float32')\n",
    "    df_samples_test[good_vars] = scaler.transform(df_samples_test[good_vars]).astype('float32')\n",
    "    \n",
    "    # Extracts features from samples\n",
    "    X_train = extract_features(df_samples_train, \n",
    "                               column_id='id', \n",
    "                               column_sort='timestamp', \n",
    "                               default_fc_parameters=df_fc_p,\n",
    "                               impute_function=impute,\n",
    "                               n_jobs=0,\n",
    "                               disable_progressbar=disable_progressbar)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = extract_features(df_samples_test, \n",
    "                              column_id='id', \n",
    "                              column_sort='timestamp',\n",
    "                              default_fc_parameters=df_fc_p,\n",
    "                              impute_function=impute,\n",
    "                              n_jobs=0,\n",
    "                              disable_progressbar=disable_progressbar)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    \n",
    "    # print(f'df_samples_train (normalizado e tratado):')\n",
    "    # display(df_samples_train)\n",
    "    # print(f'df_samples_test (normalizado):')\n",
    "    # display(df_samples_test)\n",
    "    # print(f'X_train:')\n",
    "    # display(X_train)\n",
    "    # print(f'y_train (ajustado para +1 e -1): {y_train} \\n')\n",
    "    # print(f'X_test:')\n",
    "    # display(X_test)\n",
    "    # print(f'y_test (ajustado para +1 e -1): {y_test} \\n')\n",
    "\n",
    "    # LISTA DE CLASSIFICADORES A SEREM EXPERIMENTADOS\n",
    "        \n",
    "    # DUMMY - Classificador ingênuo\n",
    "    clfs['Dummy'] = DummyClassifier(strategy='constant', constant=1)\n",
    "    print(f'\\tDummyClassifier',params)  \n",
    "    \n",
    "    # ISOLATION FOREST - busca de melhores hiperparâmetros (384 combinações)\n",
    "    isolation_forest_params = {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_samples': ['auto', 0.50, 0.75, 1.0],\n",
    "        'contamination': ['auto', 0, 0.05, 0.10],\n",
    "        'bootstrap': [True, False],\n",
    "        'max_features': [0.50, 0.75, 1.0],\n",
    "        'random_state': [random_state]\n",
    "    }\n",
    "    for params in ParameterGrid(isolation_forest_params):\n",
    "        isolation_forest_clf = IsolationForest().set_params(**params)\n",
    "        clfs[f'Floresta de Isolamento'] = isolation_forest_clf\n",
    "        #clfs[f'Floresta de Isolamento: {params}'] = isolation_forest_clf\n",
    "\n",
    "    # ONE CLASS SVM - busca de melhores hiperparâmetros (240 combinações)\n",
    "    ocsvm_params = {\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['auto', 'scale', 1e-4, 1e-3, 1e-2, 0.1, 0.50, 1.0, 5.0, 10.0], # kernel coefficient para 'rbf', 'poly' and 'sigmoid'.\n",
    "        'nu': [1e-4, 1e-3, 1e-2, 0.10, 0.50, 1.0]\n",
    "    }    \n",
    "    for params in ParameterGrid(ocsvm_params):\n",
    "        ocsvm_clf = OneClassSVM().set_params(**params)\n",
    "        #clfs[f'One-Class SVM: {params}'] = ocsvm_clf\n",
    "    \n",
    "    # LOCAL OUTLIER FACTOR (LOF) - busca de melhores hiperparâmetros (1056 combinações)\n",
    "    lof_params = {\n",
    "        'n_neighbors': [5, 10, 15, 20],\n",
    "        'algorithm': ['auto'],\n",
    "        'leaf_size': [15, 30, 45], # Leaf size passed to BallTree or KDTree algorithm. \n",
    "        'metric': ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'],\n",
    "        'contamination': ['auto', 0.01, 0.05, 0.10],\n",
    "        'novelty': [True]\n",
    "    }    \n",
    "    for params in ParameterGrid(lof_params):\n",
    "        lof_clf = LocalOutlierFactor().set_params(**params)\n",
    "        #clfs[f'Local Outlier Factor (LOF): {params}'] = lof_clf\n",
    "    \n",
    "    # ELLIPTIC ENVELOPE(LOF) - busca de melhores hiperparâmetros (36 combinações)\n",
    "    ellipticenvelope_params = {\n",
    "        'contamination': [1e-4, 1e-3, 0.01, 0.05, 0.10, 0.50],\n",
    "        'assume_centered': [True, False],\n",
    "        'support_fraction': [0.80, 0.90, 0.99], # proportion of points to be included in the support of the raw MCD estimate. \n",
    "        'random_state': [random_state]\n",
    "    }    \n",
    "    for params in ParameterGrid(ellipticenvelope_params):\n",
    "        ellipticenvelope_clf = EllipticEnvelope().set_params(**params)\n",
    "        #clfs[f'Envelope Elíptico: {params}'] = ellipticenvelope_clf\n",
    "    \n",
    "    # Treina, testa e calcula os scores para cada classificador na instância    \n",
    "    scores = train_test_calc_scores(X_train, y_train, X_test, y_test, scores, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Os resultados obtidos com os métodos implementados são apresentados abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de instâncias utilizadas: 36\n",
      "Número de instâncias ignoradas: 16\n"
     ]
    }
   ],
   "source": [
    "print(f'Número de instâncias utilizadas: {used_instances}')\n",
    "print(f'Número de instâncias ignoradas: {ignored_instances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características utilizadas: ['median', 'mean', 'standard_deviation', 'variance', 'root_mean_square', 'maximum', 'absolute_maximum', 'minimum']\n"
     ]
    }
   ],
   "source": [
    "print(f'Características utilizadas: {list(df_fc_p.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Os comandos a seguir permitem salvar e recuperar os resultados de/para um arquivo CSV de forma conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores.to_csv(r'./results/anomaly_detection_scores_por_rodada.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1. Métricas em formato tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As tabelas a seguir apresentam as médias e o desvio padrão das métricas, respectivamente. Ambos são ordenados pela medida-F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISAO</th>\n",
       "      <th>REVOGACAO</th>\n",
       "      <th>F1</th>\n",
       "      <th>TREINAMENTO [s]</th>\n",
       "      <th>TESTE [s]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIFICADOR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Floresta de Isolamento</th>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.636574</td>\n",
       "      <td>0.512285</td>\n",
       "      <td>0.119599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        PRECISAO  REVOGACAO        F1  TREINAMENTO [s]  \\\n",
       "CLASSIFICADOR                                                            \n",
       "Floresta de Isolamento  0.636574   0.636574  0.636574         0.512285   \n",
       "Dummy                   0.500000   0.500000  0.500000         0.000402   \n",
       "\n",
       "                        TESTE [s]   \n",
       "CLASSIFICADOR                       \n",
       "Floresta de Isolamento    0.119599  \n",
       "Dummy                     0.000037  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Médias\n",
    "mean_score_table = scores.groupby('CLASSIFICADOR').mean().sort_values(by=['F1'], ascending=False)\n",
    "mean_score_table.to_csv(r'./results/anomaly_detection_scores_medias.csv')\n",
    "mean_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISAO</th>\n",
       "      <th>REVOGACAO</th>\n",
       "      <th>F1</th>\n",
       "      <th>TREINAMENTO [s]</th>\n",
       "      <th>TESTE [s]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIFICADOR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00054</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PRECISAO  REVOGACAO   F1  TREINAMENTO [s]  TESTE [s] \n",
       "CLASSIFICADOR                                                       \n",
       "Dummy               0.0        0.0  0.0          0.00054    0.000247"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desvios Padrão\n",
    "std_score_table = scores.groupby('CLASSIFICADOR').std().sort_values(by=['F1'], ascending=True)\n",
    "std_score_table.to_csv(r'./results/anomaly_detection_scores_desvios_padrao.csv')\n",
    "std_score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2. Testes estatísticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Utilizado teste de Friedman e teste de Holm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Less than 2 levels",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Mestrado\\Github Wander\\[Publicação] Congresso CBA 2020.ipynb Célula: 31\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Mestrado/Github%20Wander/%5BPublica%C3%A7%C3%A3o%5D%20Congresso%20CBA%202020.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m clfs_names \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(clfs\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Mestrado/Github%20Wander/%5BPublica%C3%A7%C3%A3o%5D%20Congresso%20CBA%202020.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m f1s \u001b[39m=\u001b[39m [scores\u001b[39m.\u001b[39mloc[scores[\u001b[39m'\u001b[39m\u001b[39mCLASSIFICADOR\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39mcn, \u001b[39m'\u001b[39m\u001b[39mF1\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues \u001b[39mfor\u001b[39;00m cn \u001b[39min\u001b[39;00m clfs_names]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Mestrado/Github%20Wander/%5BPublica%C3%A7%C3%A3o%5D%20Congresso%20CBA%202020.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m f_value_stat, p_value, ranks, pivots \u001b[39m=\u001b[39m stac\u001b[39m.\u001b[39;49mfriedman_test(\u001b[39m*\u001b[39;49m(f1s))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Mestrado/Github%20Wander/%5BPublica%C3%A7%C3%A3o%5D%20Congresso%20CBA%202020.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mp_value: \u001b[39m\u001b[39m{\u001b[39;00mp_value\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Mestrado\\Github Wander\\stac\\nonparametric_tests.py:79\u001b[0m, in \u001b[0;36mfriedman_test\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m    Performs a Friedman ranking test.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39m    Tests the hypothesis that in a set of k dependent samples groups (where k >= 2) at least two of the groups represent populations with different median values.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m    D.J. Sheskin, Handbook of parametric and nonparametric statistical procedures. crc Press, 2003, Test 25: The Friedman Two-Way Analysis of Variance by Ranks\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args)\n\u001b[1;32m---> 79\u001b[0m \u001b[39mif\u001b[39;00m k \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m: \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mLess than 2 levels\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(args[\u001b[39m0\u001b[39m])\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m([\u001b[39mlen\u001b[39m(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m args])) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m: \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mUnequal number of samples\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Less than 2 levels"
     ]
    }
   ],
   "source": [
    "# Teste Estatístico (Friedman)\n",
    "\n",
    "clfs_names = list(clfs.keys())\n",
    "f1s = [scores.loc[scores['CLASSIFICADOR']==cn, 'F1'].values for cn in clfs_names]\n",
    "f_value_stat, p_value, ranks, pivots = stac.friedman_test(*(f1s))\n",
    "print(f'p_value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Teste Estatístico (Holm)\n",
    "\n",
    "comp, z_values_stat, p_values, adj_p_values = stac.holm_test(len(pivots), pivots, clfs_names, clfs_names.index('Dummy'))\n",
    "for i in range(len(comp)):\n",
    "    print(f'{comp[i]}: \\n\\tp_values: {p_values[i]}\\n\\tadj_p_values: {adj_p_values[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execução (hh:mm:ss.ms): 0:45:28.409402\n"
     ]
    }
   ],
   "source": [
    "# Calcular tempo total do notebook Jupyter\n",
    "print(f'Tempo total de execução (hh:mm:ss.ms): {datetime.now() - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Sumário",
   "title_sidebar": "Sumário",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0b5c9ea003e940675e74d523fd7e29c50e0d518b0951cfdc6ec9cc2da53d1bf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
