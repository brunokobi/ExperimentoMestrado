{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Artifício para alcular tempo total do notebook Jupyter\n",
    "from datetime import datetime \n",
    "start_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import warnings\n",
    "import sys\n",
    "sys.path.append('stac')\n",
    "import nonparametric_tests as stac\n",
    "from math import ceil\n",
    "from matplotlib import pyplot as plt\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, EfficientFCParameters\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "logging.getLogger('tsfresh').setLevel(logging.ERROR)\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "variables": {
     "import datetime;import locale;locale.setlocale(locale.LC_ALL, 'portuguese_brazil');datetime.date.today().strftime('%d de %B de %Y')": "05 de agosto de 2018"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = Path('./', 'data')\n",
    "random_state = 1\n",
    "events_names = {0: 'Normal',\n",
    "                1: 'Aumento Abrupto de BSW',\n",
    "                2: 'Fechamento Espúrio de DHSV',\n",
    "                3: 'Intermitência Severa',\n",
    "                4: 'Instabilidade de Fluxo',\n",
    "                5: 'Perda Rápida de Produtividade',\n",
    "                6: 'Restrição Rápida em CKP',\n",
    "                7: 'Incrustação em CKP',\n",
    "                8: 'Hidrato em Linha de Produção'\n",
    "               }\n",
    "vars = ['P-PDG',\n",
    "        'P-TPT',\n",
    "        'T-TPT',\n",
    "        'P-MON-CKP',\n",
    "        'T-JUS-CKP',\n",
    "        'P-JUS-CKGL',\n",
    "        'T-JUS-CKGL',\n",
    "        'QGL']\n",
    "columns = ['timestamp'] + vars + ['class'] \n",
    "normal_class_code = 0\n",
    "abnormal_classes_codes = [1, 2, 5, 6, 7, 8]\n",
    "sample_size = 3*60              # Nas observações = segundos\n",
    "min_normal_period_size = 20*60  # Nas observações = segundos\n",
    "split_range = 0.6               # Porcentagem de separação entre treino/teste\n",
    "max_samples_per_period = 15     # limitação por 'segurança'\n",
    "df_fc_p = MinimalFCParameters() # Ver documentação da biblioteca tsfresh - opção: EfficientFCParameters()\n",
    "df_fc_p.pop('sum_values')       # Remove feature inapropriada\n",
    "df_fc_p.pop('length')           # Remove feature inapropriada\n",
    "max_nan_percent = 0.1           # Para seleção de variáveis\n",
    "std_vars_min = 0.01             # Para seleção de variáveis\n",
    "clfs = {}                       # Dicionário para lista de classificadores a serem experimentados\n",
    "disable_progressbar = True      # Para menos saídas no notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def class_and_file_generator(data_path, real=False, simulated=False, drawn=False):\n",
    "    \"\"\"Gerador de lista contendo número da classe e caminho do arquivo de acordo com a fonte da instância.\"\"\"    \n",
    "    for class_path in data_path.iterdir():\n",
    "        if class_path.is_dir():\n",
    "            class_code = int(class_path.stem)\n",
    "            for instance_path in class_path.iterdir():\n",
    "                if (instance_path.suffix == '.csv'):\n",
    "                    if (simulated and instance_path.stem.startswith('SIMULATED')) or \\\n",
    "                       (drawn and instance_path.stem.startswith('DRAWN')) or \\\n",
    "                       (real and (not instance_path.stem.startswith('SIMULATED')) and \\\n",
    "                       (not instance_path.stem.startswith('DRAWN'))):\n",
    "                        yield class_code, instance_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_instance(instance_path):\n",
    "    \"\"\"Função que carrega cada instância individualmente\"\"\"\n",
    "    try:\n",
    "        well, instance_id = instance_path.stem.split('_')\n",
    "        df = pd.read_csv(instance_path, sep=',', header=0)\n",
    "        assert (df.columns == columns).all(), \\\n",
    "            f'Colunas inválidas no arquivo {str(instance_path)}: {str(df.columns.tolist())}'\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        raise Exception(f'Erro ao ler arquivo {instance_path}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_samples(df, class_code):\n",
    "    # Obtém os rótulos das observações e seu conjunto inequívoco\n",
    "    ols = list(df['class'])\n",
    "    set_ols = set()\n",
    "    for ol in ols:\n",
    "        if ol in set_ols or np.isnan(ol):\n",
    "            continue\n",
    "        set_ols.add(int(ol))       \n",
    "    \n",
    "    # Descarta os rótulos das observações e substitui todos os nan por 0\n",
    "    # (requisito da biblioteca tsfresh)\n",
    "    df_vars = df.drop('class', axis=1).fillna(0)  \n",
    "    \n",
    "    # Inicializa objetos que serão retornados\n",
    "    df_samples_train = pd.DataFrame()\n",
    "    df_samples_test = pd.DataFrame()\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "            \n",
    "    # Descubre o número máximo de amostras em períodos normais, transitórios e em regime\n",
    "    # Obtém índices (primeiro e último) sem sobreposição com outros períodos\n",
    "    f_idx = ols.index(normal_class_code)\n",
    "    l_idx = len(ols)-1-ols[::-1].index(normal_class_code)\n",
    "\n",
    "    # Define o número inicial de amostras para o período normal\n",
    "    max_samples_normal = l_idx-f_idx+1-sample_size\n",
    "    if (max_samples_normal) > 0:      \n",
    "        num_normal_samples = min(max_samples_per_period, max_samples_normal)\n",
    "        num_train_samples = int(split_range*num_normal_samples)\n",
    "        num_test_samples = num_normal_samples - num_train_samples    \n",
    "    else:\n",
    "        num_train_samples = 0\n",
    "        num_test_samples = 0\n",
    "    \n",
    "    # Define o número máximo de amostras por período transitório\n",
    "    transient_code = class_code + 100    \n",
    "    if transient_code in set_ols:\n",
    "        # Obtém índices (primeiro e último) com possível sobreposição\n",
    "        # no início do período\n",
    "        f_idx = ols.index(transient_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(transient_code)        \n",
    "        max_transient_samples = l_idx-f_idx+1-sample_size\n",
    "    else:\n",
    "        max_transient_samples = 0            \n",
    "\n",
    "    # Define o número máximo de amostras no período de regime\n",
    "    if class_code in set_ols:\n",
    "        # Obtém índices (primeiro e último) com possível sobreposição \n",
    "        # no início ou fim do período\n",
    "        f_idx = ols.index(class_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(class_code)\n",
    "        if l_idx+(sample_size-1) < len(ols)-1:\n",
    "            l_idx = l_idx+(sample_size-1) \n",
    "        else:\n",
    "            l_idx = len(ols)-1\n",
    "        max_in_regime_samples = l_idx-f_idx+1-sample_size\n",
    "    else:\n",
    "        max_in_regime_samples = 0   \n",
    "        \n",
    "    # Descubre o número adequado de amostras em períodos normais, transitórios e em regime\n",
    "    num_transient_samples = ceil(num_test_samples/2)\n",
    "    num_in_regime_samples = num_test_samples - num_transient_samples\n",
    "    if (max_transient_samples >= num_transient_samples) and \\\n",
    "       (max_in_regime_samples < num_in_regime_samples):\n",
    "        num_in_regime_samples = max_in_regime_samples        \n",
    "        num_transient_samples = min(num_test_samples-num_in_regime_samples, max_transient_samples)\n",
    "    elif (max_transient_samples < num_transient_samples) and \\\n",
    "         (max_in_regime_samples >= num_in_regime_samples):\n",
    "        num_transient_samples = max_transient_samples        \n",
    "        num_in_regime_samples = min(num_test_samples-num_transient_samples, max_in_regime_samples)\n",
    "    elif (max_transient_samples < num_transient_samples) and \\\n",
    "         (max_in_regime_samples < num_in_regime_samples):\n",
    "        num_transient_samples = max_transient_samples\n",
    "        num_in_regime_samples = max_in_regime_samples\n",
    "        num_test_samples = num_transient_samples+num_in_regime_samples\n",
    "    \n",
    "    # Extrai amostras do período normal para treinamento e teste\n",
    "    # Obtém índices (primeiro e último) sem sobreposição com outros períodos\n",
    "    f_idx = ols.index(normal_class_code)\n",
    "    l_idx = len(ols)-1-ols[::-1].index(normal_class_code)\n",
    "    \n",
    "    # Define a etapa correta e extrai amostras\n",
    "    if (num_normal_samples) > 0:  \n",
    "        if num_normal_samples == max_samples_normal:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_samples_normal-1) // (max_samples_per_period-1)\n",
    "        step_wanted = sample_size\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Extrai amostras para treinamento\n",
    "        sample_id = 0\n",
    "        for idx in range(num_train_samples):\n",
    "            f_idx_c = l_idx-sample_size+1-(num_normal_samples-1-idx)*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_train = df_samples_train.append(df_sample)\n",
    "            y_train.append(normal_class_code)\n",
    "            sample_id += 1\n",
    "    \n",
    "        # Extrai amostras para teste\n",
    "        sample_id = 0\n",
    "        for idx in range(num_train_samples, num_train_samples+num_test_samples):\n",
    "            f_idx_c = l_idx-sample_size+1-(num_normal_samples-1-idx)*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(normal_class_code)\n",
    "            sample_id += 1\n",
    "\n",
    "    # Extrai amostras do período transitório (se existir) para teste\n",
    "    if (num_transient_samples) > 0:    \n",
    "        # Define a etapa correta e extrai amostras\n",
    "        if num_transient_samples == max_transient_samples:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_transient_samples-1) // (max_samples_per_period-1)\n",
    "        step_wanted = np.inf\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Obtém índices (primeiro e último) com possível sobreposição no início deste período\n",
    "        f_idx = ols.index(transient_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(transient_code) \n",
    "\n",
    "        # Extrai amostras\n",
    "        for idx in range(num_transient_samples):\n",
    "            f_idx_c = f_idx+idx*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(transient_code)\n",
    "            sample_id += 1\n",
    "            \n",
    "    # Extrai amostras do período em regime (se existir) para teste\n",
    "    if (num_in_regime_samples) > 0:     \n",
    "        # Define a etapa correta e extrai amostras\n",
    "        if num_in_regime_samples == max_in_regime_samples:\n",
    "            step_max = 1 \n",
    "        else:\n",
    "            step_max = (max_in_regime_samples-1) // (max_samples_per_period-1)\n",
    "        step_wanted = sample_size\n",
    "        step = min(step_wanted, step_max)\n",
    "        \n",
    "        # Obtém índices (primeiro e último) com possível sobreposição \n",
    "        # no início ou no final deste período\n",
    "        f_idx = ols.index(class_code)\n",
    "        if f_idx-(sample_size-1) > 0:\n",
    "            f_idx = f_idx-(sample_size-1)\n",
    "        else:\n",
    "            f_idx = 0\n",
    "        l_idx = len(ols)-1-ols[::-1].index(class_code)\n",
    "        if l_idx+(sample_size-1) < len(ols)-1:\n",
    "            l_idx = l_idx+(sample_size-1) \n",
    "        else:\n",
    "            l_idx = len(ols)-1\n",
    "\n",
    "        # Extrai amostras\n",
    "        for idx in range(num_in_regime_samples):\n",
    "            f_idx_c = f_idx+idx*step\n",
    "            l_idx_c = f_idx_c+sample_size\n",
    "            df_sample = df_vars.iloc[f_idx_c:l_idx_c, :]\n",
    "            df_sample.insert(loc=0, column='id', value=sample_id)\n",
    "            df_samples_test = df_samples_test.append(df_sample)\n",
    "            y_test.append(class_code)\n",
    "            sample_id += 1\n",
    "            \n",
    "    #print(f'df_samples_train (antes de normalizar):')\n",
    "    #display(df_samples_train)\n",
    "    #print(f'y_train (antes de ajustar para +1 e -1): {y_train} \\n')\n",
    "    #print(f'df_samples_test (antes de normalizar):')\n",
    "    #display(df_samples_test)\n",
    "    #print(f'y_test (antes de ajustar para +1 e -1): {y_test} \\n')\n",
    "    \n",
    "    return df_samples_train, y_train, df_samples_test, y_test              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_test_calc_scores(X_train, y_train, X_test, y_test, scores, clfs):\n",
    "    X_train.reset_index(inplace=True, drop=True)\n",
    "    X_test.reset_index(inplace=True, drop=True)    \n",
    "    for clf_name, clf in clfs.items():\n",
    "        #print(f'CLASSIFICADOR: {clf_name}')\n",
    "        #print(f'y_train: {y_train}')\n",
    "        #print(f'y_test: {y_test}')\n",
    "        \n",
    "        # Treino\n",
    "        t0 = time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        t_train = time() - t0\n",
    "\n",
    "        # Teste\n",
    "        t0 = time()\n",
    "        y_pred = clf.predict(X_test)\n",
    "        #print(f'y_pred: {y_pred}')\n",
    "        t_test = time() - t0\n",
    "\n",
    "        # Plota os labels reais e preditos pelo classificador\n",
    "        \n",
    "        # fig = plt.figure(figsize=(12,1))\n",
    "        # ax = fig.add_subplot(111)\n",
    "        # plt.plot(-(y_pred), marker=11, color='orange', linestyle='') # Ordem invertida (mais natural)\n",
    "        # plt.plot(-(y_test), marker=10, color='green', linestyle='')  # Ordem invertida (mais natural)\n",
    "        # ax.grid(False)\n",
    "        # ax.set_yticks([-1, 1])\n",
    "        # ax.set_yticklabels(['Normal', 'Anormal'])\n",
    "        # ax.set_title(clf_name)            \n",
    "        # ax.set_xlabel('Amostra')\n",
    "        # ax.legend(['Classe Prevista', 'Classe Real'])\n",
    "        # plt.show()\n",
    "       \n",
    "        \n",
    "        # Calcula as metricas de desempenho\n",
    "        ret = precision_recall_fscore_support(y_test, y_pred, average='micro')\n",
    "        p, r, f1, _ = ret\n",
    "        scores = scores.append({'CLASSIFICADOR': clf_name, \n",
    "                                'PRECISAO': p,\n",
    "                                'REVOGACAO': r,\n",
    "                                'F1': f1,\n",
    "                                'TREINAMENTO [s]': t_train, \n",
    "                                'TESTE [s] ': t_test}, ignore_index=True)  \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gets all real instances but maintains only those with any type of undesirable event\n",
    "real_instances = pd.DataFrame(class_and_file_generator(data_path, \n",
    "                                                       real=True,\n",
    "                                                       simulated=False, \n",
    "                                                       drawn=False),\n",
    "                              columns=['class_code', 'instance_path'])\n",
    "real_instances = real_instances.loc[real_instances.iloc[:,0].isin(abnormal_classes_codes)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________\n",
      "Instância 1: data\\1\\WELL-00001_20140124213136.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (959)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 2: data\\1\\WELL-00002_20140126200050.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (1138)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 3: data\\1\\WELL-00006_20170801063614.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 4: data\\1\\WELL-00006_20170802123000.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 5: data\\1\\WELL-00006_20180618060245.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 6: data\\2\\WELL-00002_20131104014101.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 7: data\\2\\WELL-00003_20141122214325.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 8: data\\2\\WELL-00003_20170728150240.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 9: data\\2\\WELL-00003_20180206182917.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (586)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 10: data\\2\\WELL-00009_20170313160804.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 11: data\\2\\WELL-00010_20171218200131.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 12: data\\2\\WELL-00011_20140515110134.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 13: data\\2\\WELL-00011_20140530100015.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (482)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 14: data\\2\\WELL-00011_20140606230115.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 15: data\\2\\WELL-00011_20140720120102.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 16: data\\2\\WELL-00011_20140726180015.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (900)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 17: data\\2\\WELL-00011_20140824000118.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 18: data\\2\\WELL-00011_20140916060300.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 19: data\\2\\WELL-00011_20140921200031.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (695)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 20: data\\2\\WELL-00011_20140928100056.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 21: data\\2\\WELL-00011_20140929170028.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (975)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 22: data\\2\\WELL-00011_20140929220121.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 23: data\\2\\WELL-00011_20141005170056.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 24: data\\2\\WELL-00011_20141006160121.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 25: data\\2\\WELL-00012_20170320033022.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (773)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 26: data\\2\\WELL-00012_20170320143144.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 27: data\\2\\WELL-00013_20170329020229.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 28: data\\5\\WELL-00015_20170620160349.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 29: data\\5\\WELL-00015_20171013140047.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 30: data\\5\\WELL-00016_20180405020345.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (1145)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 31: data\\5\\WELL-00016_20180426142005.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (321)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 32: data\\5\\WELL-00016_20180426145108.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (376)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 33: data\\5\\WELL-00016_20180517222322.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 34: data\\5\\WELL-00017_20140314180000.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (0)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 35: data\\5\\WELL-00017_20140317151743.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 36: data\\5\\WELL-00017_20140318023141.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 37: data\\5\\WELL-00017_20140318160220.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 38: data\\5\\WELL-00017_20140319040453.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 39: data\\5\\WELL-00017_20140319141450.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 40: data\\6\\WELL-00002_20140212170333.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 41: data\\6\\WELL-00002_20140301151700.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 42: data\\6\\WELL-00002_20140325170304.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 43: data\\6\\WELL-00004_20171031181509.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 44: data\\6\\WELL-00004_20171031193025.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (414)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 45: data\\6\\WELL-00004_20171031200059.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (845)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 46: data\\7\\WELL-00001_20170226220309.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 47: data\\7\\WELL-00006_20180618110721.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 48: data\\7\\WELL-00006_20180620181348.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 49: data\\7\\WELL-00018_20180611040207.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (0)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 50: data\\8\\WELL-00019_20170301182317.csv\n",
      "____________________________________________________________________________________\n",
      "Instância 51: data\\8\\WELL-00020_20120410192326.csv\n",
      "\tignorado porque normal_period_size é insuficiente para treinamento (173)\n",
      "\n",
      "____________________________________________________________________________________\n",
      "Instância 52: data\\8\\WELL-00021_20170509013517.csv\n"
     ]
    }
   ],
   "source": [
    "# For each real instance with any type of undesirable event\n",
    "scores = pd.DataFrame()\n",
    "ignored_instances = 0\n",
    "used_instances = 10\n",
    "for i, row in real_instances.iterrows():    \n",
    "    # Loads the current instance\n",
    "    class_code, instance_path = row\n",
    "    print(f'____________________________________________________________________________________')        \n",
    "    print(f'Instância {i+1}: {instance_path}')\n",
    "    df = load_instance(instance_path)\n",
    "    \n",
    "    # Ignores instances without sufficient normal periods\n",
    "    #Ignora instâncias sem períodos normais suficientes\n",
    "    normal_period_size = (df['class']==float(normal_class_code)).sum()\n",
    "   \n",
    "    if normal_period_size < min_normal_period_size:\n",
    "        ignored_instances += 1\n",
    "        print(f'\\tignorado porque normal_period_size é insuficiente para treinamento ({normal_period_size})\\n')\n",
    "        continue\n",
    "    used_instances += 1\n",
    "        \n",
    "    # Extracts samples from the current real instance\n",
    "    # Extrai amostras da instância real atual\n",
    "    df_samples_train, y_train, df_samples_test, y_test = extract_samples(df, class_code)\n",
    "    \n",
    "\n",
    "    # Changes types of the labels (tsfresh's requirement)\n",
    "    # Altera os tipos de rótulos (requisito do tsfresh)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "       \n",
    "   \n",
    "    # (Wander) incluidas as duas linhas abaixo para ajustar y_train também (?\n",
    "    y_train[y_train!=normal_class_code] = -1\n",
    "    y_train[y_train==normal_class_code] = 1\n",
    "\n",
    "    # We want binary classification: 1 for inliers (negative class = normal instance) and\n",
    "    # Queremos classificação binária: 1 para inliers (classe negativa = instância normal) e\n",
    "    # -1 for outliers (positive class = instance with anomaly) (sklearn's requirement)\n",
    "    # -1 para anomalias (classe positiva = instância com anomalia) (requisito do sklearn)\n",
    "    y_test[y_test!=normal_class_code] = -1\n",
    "    y_test[y_test==normal_class_code] = 1\n",
    "    \n",
    "    # Drops the bad vars\n",
    "    # Elimina as vars ruins\n",
    "    good_vars = np.isnan(df_samples_train[vars]).mean(0) <= max_nan_percent\n",
    "    std_vars = np.nanstd(df_samples_train[vars], 0)\n",
    "    good_vars &= (std_vars > std_vars_min)    \n",
    "    good_vars = list(good_vars.index[good_vars])\n",
    "    bad_vars = list(set(vars)-set(good_vars))\n",
    "    df_samples_train.drop(columns=bad_vars, inplace=True, errors='ignore')\n",
    "    df_samples_test.drop(columns=bad_vars, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Normalizes the samples (zero mean and unit variance)\n",
    "    # Normaliza as amostras (média zero e variância unitária)\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    df_samples_train[good_vars] = scaler.fit_transform(df_samples_train[good_vars]).astype('float32')\n",
    "    df_samples_test[good_vars] = scaler.transform(df_samples_test[good_vars]).astype('float32')\n",
    "    \n",
    "    # Extracts features from samples\n",
    "    # Extrai recursos de amostras\n",
    "    X_train = extract_features(df_samples_train, \n",
    "                               column_id='id', \n",
    "                               column_sort='timestamp', \n",
    "                               default_fc_parameters=df_fc_p,\n",
    "                               impute_function=impute,\n",
    "                               n_jobs=0,\n",
    "                               disable_progressbar=disable_progressbar)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = extract_features(df_samples_test, \n",
    "                              column_id='id', \n",
    "                              column_sort='timestamp',\n",
    "                              default_fc_parameters=df_fc_p,\n",
    "                              impute_function=impute,\n",
    "                              n_jobs=0,\n",
    "                              disable_progressbar=disable_progressbar)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    \n",
    "    # print(f'df_samples_train (normalizado e tratado):')\n",
    "    # display(df_samples_train)\n",
    "    # print(f'df_samples_test (normalizado):')\n",
    "    # display(df_samples_test)\n",
    "    # print(f'X_train:')\n",
    "    # display(X_train)\n",
    "    # print(f'y_train (ajustado para +1 e -1): {y_train} \\n')\n",
    "    # print(f'X_test:')\n",
    "    # display(X_test)\n",
    "    # print(f'y_test (ajustado para +1 e -1): {y_test} \\n')\n",
    "\n",
    "    # LISTA DE CLASSIFICADORES A SEREM EXPERIMENTADOS\n",
    "        \n",
    "    # DUMMY - Classificador ingênuo\n",
    "    clfs['Dummy'] = DummyClassifier(strategy='constant', constant=1)    \n",
    "    \n",
    "    # ISOLATION FOREST - busca de melhores hiperparâmetros (384 combinações)\n",
    "    isolation_forest_params = {\n",
    "        # 'n_estimators': [50, 100, 150, 200],\n",
    "        # 'max_samples': ['auto', 0.50, 0.75, 1.0],\n",
    "        # 'contamination': ['auto', 0, 0.05, 0.10],\n",
    "        # 'bootstrap': [True, False],\n",
    "        # 'max_features': [0.50, 0.75, 1.0],\n",
    "        'n_estimators': [100],\n",
    "        'max_samples': [0.50],\n",
    "        'contamination': [0.000000001],\n",
    "        'bootstrap': [False],\n",
    "        'max_features': [0.75],\n",
    "        'random_state': [random_state]\n",
    "    }\n",
    "    for params in ParameterGrid(isolation_forest_params):\n",
    "        isolation_forest_clf = IsolationForest().set_params(**params)       \n",
    "        clfs[f'Floresta de Isolamento: {params}'] = isolation_forest_clf\n",
    "\n",
    "    # ONE CLASS SVM - busca de melhores hiperparâmetros (240 combinações)\n",
    "    ocsvm_params = {\n",
    "        # 'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "        # 'gamma': ['auto', 'scale', 1e-4, 1e-3, 1e-2, 0.1, 0.50, 1.0, 5.0, 10.0], # kernel coefficient para 'rbf', 'poly' and 'sigmoid'.\n",
    "        # 'nu': [1e-4, 1e-3, 1e-2, 0.10, 0.50, 1.0]\n",
    "        'kernel': ['sigmoid'],\n",
    "        'gamma': [1e-2], # kernel coefficient para 'rbf', 'poly' and 'sigmoid'.\n",
    "        'nu': [1e-3]\n",
    "        \n",
    "    }    \n",
    "    for params in ParameterGrid(ocsvm_params):\n",
    "        ocsvm_clf = OneClassSVM().set_params(**params)\n",
    "        clfs[f'One-Class SVM: {params}'] = ocsvm_clf\n",
    "        \n",
    "    \n",
    "    # LOCAL OUTLIER FACTOR (LOF) - busca de melhores hiperparâmetros (1056 combinações)\n",
    "    lof_params = {\n",
    "        # 'n_neighbors': [5, 10, 15, 20],\n",
    "        # 'algorithm': ['auto'],\n",
    "        # 'leaf_size': [15, 30, 45], # Leaf size passed to BallTree or KDTree algorithm. \n",
    "        # 'metric': ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan', 'braycurtis',\n",
    "        #  'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'minkowski',\n",
    "        #  'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'],\n",
    "        # 'contamination': ['auto', 0.01, 0.05, 0.10],\n",
    "        'n_neighbors': [20],\n",
    "        'algorithm': ['auto'],\n",
    "        'leaf_size': [15], # Leaf size passed to BallTree or KDTree algorithm. \n",
    "        'metric': ['chebyshev'],\n",
    "        'contamination': ['auto'],\n",
    "        'novelty': [True]\n",
    "    }    \n",
    "    for params in ParameterGrid(lof_params):\n",
    "        lof_clf = LocalOutlierFactor().set_params(**params)\n",
    "        clfs[f'Local Outlier Factor (LOF): {params}'] = lof_clf\n",
    "       \n",
    "    \n",
    "    # ELLIPTIC ENVELOPE(LOF) - busca de melhores hiperparâmetros (36 combinações)\n",
    "    ellipticenvelope_params = {\n",
    "        # 'contamination': [1e-4, 1e-3, 0.01, 0.05, 0.10, 0.50],\n",
    "        # 'assume_centered': [True, False],\n",
    "        # 'support_fraction': [0.80, 0.90, 0.99], # proportion of points to be included in the support of the raw MCD estimate.\n",
    "        'contamination': [0.01],\n",
    "        'assume_centered': [False],\n",
    "        'support_fraction': [0.99], # proportion of points to be included in the support of the raw MCD estimate. \n",
    "        'random_state': [random_state]\n",
    "    }    \n",
    "    for params in ParameterGrid(ellipticenvelope_params):\n",
    "        ellipticenvelope_clf = EllipticEnvelope().set_params(**params)\n",
    "        clfs[f'Envelope Elíptico: {params}'] = ellipticenvelope_clf\n",
    "        \n",
    "    \n",
    "    # Treina, testa e calcula os scores para cada classificador na instância    \n",
    "    scores = train_test_calc_scores(X_train, y_train, X_test, y_test, scores, clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Os resultados obtidos com os métodos implementados são apresentados abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de instâncias utilizadas: 46\n",
      "Número de instâncias ignoradas: 16\n"
     ]
    }
   ],
   "source": [
    "print(f'Número de instâncias utilizadas: {used_instances}')\n",
    "print(f'Número de instâncias ignoradas: {ignored_instances}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características utilizadas: ['median', 'mean', 'standard_deviation', 'variance', 'root_mean_square', 'maximum', 'absolute_maximum', 'minimum']\n"
     ]
    }
   ],
   "source": [
    "print(f'Características utilizadas: {list(df_fc_p.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Os comandos a seguir permitem salvar e recuperar os resultados de/para um arquivo CSV de forma conveniente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores.to_csv(r'./results/anomaly_detection_scores_por_rodada.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1. Métricas em formato tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As tabelas a seguir apresentam as médias e o desvio padrão das métricas, respectivamente. Ambos são ordenados pela medida-F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISAO</th>\n",
       "      <th>REVOGACAO</th>\n",
       "      <th>F1</th>\n",
       "      <th>TREINAMENTO [s]</th>\n",
       "      <th>TESTE [s]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIFICADOR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Local Outlier Factor (LOF): {'algorithm': 'auto', 'contamination': 'auto', 'leaf_size': 15, 'metric': 'chebyshev', 'n_neighbors': 20, 'novelty': True}</th>\n",
       "      <td>0.881944</td>\n",
       "      <td>0.881944</td>\n",
       "      <td>0.881944</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>0.006917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Floresta de Isolamento: {'bootstrap': False, 'contamination': 1e-09, 'max_features': 0.75, 'max_samples': 0.5, 'n_estimators': 100, 'random_state': 1}</th>\n",
       "      <td>0.715278</td>\n",
       "      <td>0.715278</td>\n",
       "      <td>0.715278</td>\n",
       "      <td>0.232226</td>\n",
       "      <td>0.057306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One-Class SVM: {'gamma': 0.01, 'kernel': 'sigmoid', 'nu': 0.001}</th>\n",
       "      <td>0.476852</td>\n",
       "      <td>0.476852</td>\n",
       "      <td>0.476852</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.001942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    PRECISAO  REVOGACAO  \\\n",
       "CLASSIFICADOR                                                             \n",
       "Local Outlier Factor (LOF): {'algorithm': 'auto...  0.881944   0.881944   \n",
       "Floresta de Isolamento: {'bootstrap': False, 'c...  0.715278   0.715278   \n",
       "Dummy                                               0.500000   0.500000   \n",
       "One-Class SVM: {'gamma': 0.01, 'kernel': 'sigmo...  0.476852   0.476852   \n",
       "\n",
       "                                                          F1  TREINAMENTO [s]  \\\n",
       "CLASSIFICADOR                                                                   \n",
       "Local Outlier Factor (LOF): {'algorithm': 'auto...  0.881944         0.006557   \n",
       "Floresta de Isolamento: {'bootstrap': False, 'c...  0.715278         0.232226   \n",
       "Dummy                                               0.500000         0.000250   \n",
       "One-Class SVM: {'gamma': 0.01, 'kernel': 'sigmo...  0.476852         0.002307   \n",
       "\n",
       "                                                    TESTE [s]   \n",
       "CLASSIFICADOR                                                   \n",
       "Local Outlier Factor (LOF): {'algorithm': 'auto...    0.006917  \n",
       "Floresta de Isolamento: {'bootstrap': False, 'c...    0.057306  \n",
       "Dummy                                                 0.000000  \n",
       "One-Class SVM: {'gamma': 0.01, 'kernel': 'sigmo...    0.001942  "
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Médias\n",
    "mean_score_table = scores.groupby('CLASSIFICADOR').mean().sort_values(by=['F1'], ascending=False)\n",
    "mean_score_table.to_csv(r'./results/anomaly_detection_scores_medias.csv')\n",
    "mean_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISAO</th>\n",
       "      <th>REVOGACAO</th>\n",
       "      <th>F1</th>\n",
       "      <th>TREINAMENTO [s]</th>\n",
       "      <th>TESTE [s]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CLASSIFICADOR</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Local Outlier Factor (LOF): {'algorithm': 'auto', 'contamination': 'auto', 'leaf_size': 15, 'metric': 'chebyshev', 'n_neighbors': 20, 'novelty': True}</th>\n",
       "      <td>0.126577</td>\n",
       "      <td>0.126577</td>\n",
       "      <td>0.126577</td>\n",
       "      <td>0.003184</td>\n",
       "      <td>0.001874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>One-Class SVM: {'gamma': 0.01, 'kernel': 'sigmoid', 'nu': 0.001}</th>\n",
       "      <td>0.153802</td>\n",
       "      <td>0.153802</td>\n",
       "      <td>0.153802</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Floresta de Isolamento: {'bootstrap': False, 'contamination': 1e-09, 'max_features': 0.75, 'max_samples': 0.5, 'n_estimators': 100, 'random_state': 1}</th>\n",
       "      <td>0.176355</td>\n",
       "      <td>0.176355</td>\n",
       "      <td>0.176355</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>0.004319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    PRECISAO  REVOGACAO  \\\n",
       "CLASSIFICADOR                                                             \n",
       "Dummy                                               0.000000   0.000000   \n",
       "Local Outlier Factor (LOF): {'algorithm': 'auto...  0.126577   0.126577   \n",
       "One-Class SVM: {'gamma': 0.01, 'kernel': 'sigmo...  0.153802   0.153802   \n",
       "Floresta de Isolamento: {'bootstrap': False, 'c...  0.176355   0.176355   \n",
       "\n",
       "                                                          F1  TREINAMENTO [s]  \\\n",
       "CLASSIFICADOR                                                                   \n",
       "Dummy                                               0.000000         0.000439   \n",
       "Local Outlier Factor (LOF): {'algorithm': 'auto...  0.126577         0.003184   \n",
       "One-Class SVM: {'gamma': 0.01, 'kernel': 'sigmo...  0.153802         0.000671   \n",
       "Floresta de Isolamento: {'bootstrap': False, 'c...  0.176355         0.015286   \n",
       "\n",
       "                                                    TESTE [s]   \n",
       "CLASSIFICADOR                                                   \n",
       "Dummy                                                 0.000000  \n",
       "Local Outlier Factor (LOF): {'algorithm': 'auto...    0.001874  \n",
       "One-Class SVM: {'gamma': 0.01, 'kernel': 'sigmo...    0.000474  \n",
       "Floresta de Isolamento: {'bootstrap': False, 'c...    0.004319  "
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Desvios Padrão\n",
    "std_score_table = scores.groupby('CLASSIFICADOR').std().sort_values(by=['F1'], ascending=True)\n",
    "std_score_table.to_csv(r'./results/anomaly_detection_scores_desvios_padrao.csv')\n",
    "std_score_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2. Testes estatísticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Utilizado teste de Friedman e teste de Holm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value: 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "# Teste Estatístico (Friedman)\n",
    "\n",
    "clfs_names = list(clfs.keys())\n",
    "f1s = [scores.loc[scores['CLASSIFICADOR']==cn, 'F1'].values for cn in clfs_names]\n",
    "f_value_stat, p_value, ranks, pivots = stac.friedman_test(*(f1s))\n",
    "print(f'p_value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy vs Local Outlier Factor (LOF): {'algorithm': 'auto', 'contamination': 'auto', 'leaf_size': 15, 'metric': 'chebyshev', 'n_neighbors': 20, 'novelty': True}: \n",
      "\tp_values: 1.0397682714824441e-11\n",
      "\tadj_p_values: 3.119304814447332e-11\n",
      "Dummy vs Floresta de Isolamento: {'bootstrap': False, 'contamination': 1e-09, 'max_features': 0.75, 'max_samples': 0.5, 'n_estimators': 100, 'random_state': 1}: \n",
      "\tp_values: 0.0005225753951243473\n",
      "\tadj_p_values: 0.0010451507902486945\n",
      "Dummy vs One-Class SVM: {'gamma': 0.01, 'kernel': 'sigmoid', 'nu': 0.001}: \n",
      "\tp_values: 0.8194769767775212\n",
      "\tadj_p_values: 0.8194769767775212\n"
     ]
    }
   ],
   "source": [
    "# Teste Estatístico (Holm)\n",
    "\n",
    "comp, z_values_stat, p_values, adj_p_values = stac.holm_test(len(pivots), pivots, clfs_names, clfs_names.index('Dummy'))\n",
    "for i in range(len(comp)):\n",
    "    print(f'{comp[i]}: \\n\\tp_values: {p_values[i]}\\n\\tadj_p_values: {adj_p_values[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo total de execução (hh:mm:ss.ms): 0:00:22.780658\n"
     ]
    }
   ],
   "source": [
    "# Calcular tempo total do notebook Jupyter\n",
    "print(f'Tempo total de execução (hh:mm:ss.ms): {datetime.now() - start_time}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Sumário",
   "title_sidebar": "Sumário",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "0b5c9ea003e940675e74d523fd7e29c50e0d518b0951cfdc6ec9cc2da53d1bf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
